{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "#from sklearn.ensemble import RandomForestClassifier\n",
    "#from sklearn.linear_model import LogisticRegression\n",
    "#from sklearn.svm import LinearSVC\n",
    "#from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = ['Text Message', 'Category']\n",
    "messages = pd.read_csv('To_Train/Emails.csv', skipinitialspace=True, usecols=fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Text Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>90</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>6</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Personal_Info_Update</td>\n",
       "      <td>Hello Team,\\n\\nThe subject request is not proc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Category  \\\n",
       "count                     90   \n",
       "unique                     6   \n",
       "top     Personal_Info_Update   \n",
       "freq                      15   \n",
       "\n",
       "                                             Text Message  \n",
       "count                                                  90  \n",
       "unique                                                 62  \n",
       "top     Hello Team,\\n\\nThe subject request is not proc...  \n",
       "freq                                                    5  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_process(mess):\n",
    "    \"\"\"\n",
    "    Takes in a string of text, then performs the following:\n",
    "    1. Remove all punctuation\n",
    "    2. Remove all stopwords\n",
    "    3. Returns a list of the cleaned text\n",
    "    \"\"\"\n",
    "    # Check characters to see if they are in punctuation\n",
    "    nopunc = [char for char in mess if char not in string.punctuation]\n",
    "\n",
    "    # Join the characters again to form the string.\n",
    "    nopunc = ''.join(nopunc)\n",
    "    \n",
    "    # Now just remove any stopwords\n",
    "    return [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39    Team,\\n\\nThe subject request is not processed ...\n",
       "85    Hi Team,\\n\\nThe subjected customer has request...\n",
       "78    Dear Team,\\n\\nThe subjected customer has reque...\n",
       "58    Dear Team,\\n\\nThe customer has requested to up...\n",
       "71    Hi Team,\\n\\nThe customer request is for her ne...\n",
       "41    Hello Team,\\n\\nThe subject request is not proc...\n",
       "76    Dear Team,\\n\\nThe subjected customer has reque...\n",
       "3     Dear Team,\\n\\nThe below mentioned customer is ...\n",
       "5     Dear Team,\\n\\nThe below mentioned customer is ...\n",
       "43    Hi Team,\\n\\nThe subject request is not process...\n",
       "67    Dear Team,\\n\\nThe customer request is for her ...\n",
       "16    Team,\\n\\nThe below mentioned customer is reque...\n",
       "75    Team,\\n\\nThe subjected customer has requested ...\n",
       "15    Dear Team,\\n\\nThe below mentioned customer is ...\n",
       "6     Hi,\\n\\nThe below mentioned customer is request...\n",
       "30    Team,\\n\\nThe subject request is not processed ...\n",
       "70    Hello Team,\\n\\nThe customer request is for his...\n",
       "66    Team,\\n\\nThe customer request is for his new e...\n",
       "Name: Text Message, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg_train, msg_test, label_train, label_test = \\\n",
    "train_test_split(messages['Text Message'], messages['Category'], test_size=0.2)\n",
    "\n",
    "#print(len(msg_train), len(msg_test), len(msg_train) + len(msg_test))\n",
    "msg_test#, label_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('bow', CountVectorizer(analyzer=text_process)),  # strings to token integer counts\n",
    "    ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n",
    "    ('classifier', MultinomialNB()),  # train on TF-IDF vectors w/ Naive Bayes classifier\n",
    "])\n",
    "#NaiveBayes - BinominalNB(), MultiNominalNB(), GaussianNB()\n",
    "#RandomForestClassifier() \n",
    "#LogisticRegression()\n",
    "#LinearSVC()\n",
    "#SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('bow', CountVectorizer(analyzer=<function text_process at 0x000001D68FD226A8>,\n",
       "        binary=False, decode_error='strict', dtype=<class 'numpy.int64'>,\n",
       "        encoding='utf-8', input='content', lowercase=True, max_df=1.0,\n",
       "        max_features=None, min_df=1, ngram_range=(1, 1), preprocesso...f=False, use_idf=True)), ('classifier', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(msg_train,label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pipeline.predict(msg_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              precision    recall  f1-score   support\n",
      "\n",
      "             Complaints_Personal_Info_Update       1.00      1.00      1.00         1\n",
      "               FollowUp_Personal_Info_Update       1.00      1.00      1.00         4\n",
      "                        Personal_Info_Update       1.00      1.00      1.00         3\n",
      "Social_Media_Complaints_Personal_Info_Update       1.00      1.00      1.00         4\n",
      "                   Supplementary_Card_Update       1.00      1.00      1.00         2\n",
      "                 Urgent_Personal_Info_Update       1.00      1.00      1.00         4\n",
      "\n",
      "                                 avg / total       1.00      1.00      1.00        18\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print(classification_report(label_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print (label_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in predictions:\n",
    "#    print (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['EMail_Class_Model.pkl']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model as a pickle in a file \n",
    "joblib.dump(pipeline, 'EMail_Class_Model.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model from the file \n",
    "pipeline_from_joblib = joblib.load('EMail_Class_Model.pkl')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FollowUp_Personal_Info_Update\n",
      "Social_Media_Complaints_Personal_Info_Update\n",
      "Social_Media_Complaints_Personal_Info_Update\n",
      "Complaints_Personal_Info_Update\n",
      "Urgent_Personal_Info_Update\n",
      "FollowUp_Personal_Info_Update\n",
      "Social_Media_Complaints_Personal_Info_Update\n",
      "Personal_Info_Update\n",
      "Personal_Info_Update\n",
      "FollowUp_Personal_Info_Update\n",
      "Urgent_Personal_Info_Update\n",
      "Supplementary_Card_Update\n",
      "Social_Media_Complaints_Personal_Info_Update\n",
      "Supplementary_Card_Update\n",
      "Personal_Info_Update\n",
      "FollowUp_Personal_Info_Update\n",
      "Urgent_Personal_Info_Update\n",
      "Urgent_Personal_Info_Update\n"
     ]
    }
   ],
   "source": [
    "# Use the loaded model to make predictions \n",
    "#pipeline_from_joblib.predict(msg_test) \n",
    "#for i in pipeline_from_joblib.predict(msg_test):\n",
    "#    print (i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
